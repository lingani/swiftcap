{
    "contents" : "###############################################\n## Synopsis\n###############################################\n# Here we present the first milestone report of the Data Science Capstone Project.\n# This report will cover the three first tasks that are:\n# Task 1: Understanding the Problem;\n# Task 2: Data acquisition and cleaning;\n# Task 3: Initial Exploratory Analysis. \n# Packages we used and choices we made will be explaned \n# and questions regarding the next steps of the project will come out.\n\n\n###############################################\n## Task 1: Understanding the Problem;\n###############################################\n# The question is: how can we predict the next word when someone is texting. Have a looke at SWIFTKEY to see haw it works and how it is \n# helpful for rapid text completion. Well this project will build the next word predictive algorithm based on a dataset provided by \n# SWIFTKEY. Word frequency, patterns (conacataneted Tokens) in those files will help us predict the new word. SWIFTKEY provedes data for \n# three languages but English will reatain our attention here.\n# We all know that when it comes to prediction, there is no absolute accuracy, but we can try to approach the solution as much as possible.\n# Many packages specially flom NLP familly will be used.\n\n\n\n###############################################\n## Task 2: Data acquisition and sampling, tokenization and profanity filtering\n###############################################\n# Download the data from Coursera website and drop unzip it on a dirctory colled data. Mine is in a directory\n# ../Project/Data. Let's set those directories as Working and Data Directories of the project.\n\n# Data acquisition and sampling\n# settig working directory and the data Directory\nsetwd(\"F:/Cours/Coursera/The Data Science Specialization/Module 10 - Capstone Project/Project\")\ndataDir<-\"./data\"\n\n# Reading and sampling en_US Data\n# The Function bellow is used to read and sample the data\nreading_and_sampling <- function(data_URL, percentage, prob, sampled_name){\n  set.seed(12345)\n  cnx <- file(data_URL, \"r\")\n  data <- readLines(cnx, encoding = \"UTF-8\")\n  data <- data[rbinom(length(data)*percentage, length(data), prob)]\n  filepath <- paste0(dataDir, \"/sampled/\", sampled_name, \".txt\")  \n  fileConn<-file(filepath)\n  writeLines(data, fileConn)\n  close(fileConn)\n  data\n}\n\n# Here we call 'reading_and_sampling' function on the data\nprob <- 0.3\npercentage <- 0.03\n\nen_US.twitter_URL<-paste0(dataDir,\"/final/en_US/en_US.twitter.txt\" )\ndata_twitter <- reading_and_sampling(en_US.twitter_URL, percentage, prob, \"sampled_twitter\")\n\nen_US.blogs_URL<-paste0(dataDir,\"/final/en_US/en_US.blogs.txt\" )\ndata_blogs <- reading_and_sampling(en_US.blogs_URL, percentage, prob, \"sampled_blogs\")\n\nen_US.news_URL<-paste0(dataDir,\"/final/en_US/en_US.news.txt\" )\ndata_news <- reading_and_sampling(en_US.news_URL, percentage, prob, \"sampled_news\")\n\n\n### Tokenization and Profanity Fiiltering\nlibrary(openNLP)\nlibrary(tm)\nlibrary(qdap)\nlibrary(RWeka)\n\n### creating Corpus for datasets\ncreating_corpus <- function(data){\n  sentences <- sent_detect(data, language = \"en\", model = NULL)\n  corpus <- VCorpus(VectorSource(sentences)) # Building corpus with sampled data\n  corpus <- tm_map(corpus, removeNumbers) # Removing numbers\n  corpus <- tm_map(corpus, stripWhitespace) # Removing whitespaces\n  corpus <- tm_map(corpus, content_transformer(tolower)) #Lowercasing all contents\n  corpus <- tm_map(corpus, removePunctuation) # Removing special characters\n  corpus\n}\n\ncorpus_twitter <- creating_corpus(data_twitter)\ncorpus_blogs <- creating_corpus(data_blogs)\ncorpus_news <- creating_corpus(data_news)\n\ndf_twitter <-data.frame(text=unlist(sapply(corpus_twitter, `[`, \"content\")), stringsAsFactors=F)\ndf_blogs <-data.frame(text=unlist(sapply(corpus_blogs, `[`, \"content\")), stringsAsFactors=F)\ndf_news <-data.frame(text=unlist(sapply(corpus_news, `[`, \"content\")), stringsAsFactors=F)\n\n\n\n# Using the RWeka package for the single word tokenization, Bi-grams sets \n# and Tri-grams sets for further Exploratory Analysis, keeping each in a separate list for now.\n\nOneToken_twitter <- NGramTokenizer(df_twitter, Weka_control(min = 1, max = 1))\nBiToken_blogs <- NGramTokenizer(df_twitter, Weka_control(min = 2, max = 2))\nTriToken_twitter <- NGramTokenizer(df_twitter, Weka_control(min = 3, max = 3))\n\nOneToken_blogs <- NGramTokenizer(df_blogs, Weka_control(min = 1, max = 1))\nBiToken_blogs <- NGramTokenizer(df_blogs, Weka_control(min = 2, max = 2))\nTriToken_blogs <- NGramTokenizer(df_blogs, Weka_control(min = 3, max = 3))\n\nOneToken_news <- NGramTokenizer(df_news, Weka_control(min = 1, max = 1))\nBiToken_news <- NGramTokenizer(df_news, Weka_control(min = 2, max = 2))\nTriToken_news <- NGramTokenizer(df_news, Weka_control(min = 3, max = 3))\n\n\n\n########################################################################################################\n##### Task 3: Initial Exploratory Analysis\n#######################################################################################################\n\n# Building dataframes converting tokens of n-grams into tables\nTable_OneToken_twitter <- data.frame(table(OneToken_twitter))\nTable_BiToken_twitter <- data.frame(table(BiToken_twitter))\nTable_TriToken_twitter <- data.frame(table(TriToken_twitter))\n\nTable_OneToken_blogs <- data.frame(table(OneToken_blogs))\nTable_BitwoToken_blogs <- data.frame(table(BiToken_blogs))\nTable_TriToken_blogs <- data.frame(table(TriToken_blogs))\n\nTable_OneToken_news <- data.frame(table(OneToken_news))\nTable_BwoToken_news <- data.frame(table(BiToken_news))\nTable_TriToken_news <- data.frame(table(TriToken_news))\n\n# Because loading and processing the data take time we are saving the data tables above for future use \n# Here is a simple function that saves the data\nsave_data <- function(data, file_name){\n  filepath <- paste0(dataDir, \"/processed/\", file_name, \".csv\")  \n  write.table(data, filepath)\n  data\n}\n\n# Saving twitter_data\nsave_data(Table_OneToken_twitter, \"Table_OneToken_twitter\")\nsave_data(Table_BiToken_twitter, \"Table_BiToken_twitter\")\nsave_data(Table_TriToken_twitter, \"Table_TriToken_twitter\")\n\n# Saving blogs data\nsave_data(Table_OneToken_blogs, \"Table_OneToken_blogs\")\nsave_data(Table_BiToken_blogs, \"Table_BiToken_blogs\")\nsave_data(Table_TriToken_blogs, \"Table_TriToken_blogs\")\n\n# Saving news data\nsave_data(Table_OneToken_news, \"Table_OneToken_news\")\nsave_data(Table_BiToken_news, \"Table_BiToken_news\")\nsave_data(Table_TriToken_news, \"Table_TriToken_news\")\n\n# Before ploting Token frequences, let's order them.\n# The 10 most frequent patterns will be plotted \nOrderedOneToken_twitter <- Table_OneToken_twitter[order(Table_OneToken_twitter$Freq,decreasing = TRUE),]\nOrderedBiToken_twitter <- Table_BiToken_twitter[order(Table_BiToken_twitter$Freq,decreasing = TRUE),]\nOrderedTriToken_twitter <- Table_TriToken_twitter[order(Table_TriToken_twitter$Freq,decreasing = TRUE),]\n\nOrderedOneToken_blogs <- Table_OneToken_twitter[order(Table_OneToken_blogs$Freq,decreasing = TRUE),]\nOrderedBiToken_blogs <- Table_BiToken_twitter[order(Table_BiToken_blogs$Freq,decreasing = TRUE),]\nOrderedTriToken_blogs <- Table_TriToken_twitter[order(Table_TriToken_blogs$Freq,decreasing = TRUE),]\n\n\nOrderedOneToken_news <- Table_OneToken_twitter[order(Table_OneToken_news$Freq,decreasing = TRUE),]\nOrderedBiToken_news <- Table_BiToken_twitter[order(Table_BiToken_news$Freq,decreasing = TRUE),]\nOrderedTriToken_news <- Table_TriToken_twitter[order(Table_TriToken_news$Freq,decreasing = TRUE),]\n\n# Here we make the sampling of the top 10 n-grams n from 1 to 3 for all three datasets.\nMostFreq_OneToken_twitter <- OrderedOneToken_twitter[1:10,]; colnames(MostFreq_OneToken_twitter) <- c(\"Word\",\"Frequency\")\nMostFreq_BiToken_twitter <- OrderedBiToken_twitter[1:10,]; colnames(MostFreq_OneToken_twitter) <- c(\"Word\",\"Frequency\")\nMostFreq_TriToken_twitter <- OrderedTriToken_twitter[1:10,]; colnames(MostFreq_OneToken_twitter) <- c(\"Word\",\"Frequency\")\n\nMostFreq_OneToken_blogs <- OrderedOneToken_blogs[1:10,]; colnames(MostFreq_OneToken_blogsr) <- c(\"Word\",\"Frequency\")\nMostFreq_BiToken_blogs <- OrderedBiToken_blogs[1:10,]; colnames(MostFreq_OneToken_blogs) <- c(\"Word\",\"Frequency\")\nMostFreq_TriToken_blogs <- OrderedTriToken_blogs[1:10,]; colnames(MostFreq_OneToken_blogs) <- c(\"Word\",\"Frequency\")\n\nMostFreq_OneToken_news <- OrderedOneToken_news[1:10,]; colnames(MostFreq_OneToken_news) <- c(\"Word\",\"Frequency\")\nMostFreq_BiToken_news <- OrderedBiToken_news[1:10,]; colnames(MostFreq_OneToken_news) <- c(\"Word\",\"Frequency\")\nMostFreq_TriToken_news <- OrderedTriToken_news[1:10,]; colnames(MostFreq_OneToken_news) <- c(\"Word\",\"Frequency\")\n\n########### charts\nrequire(gridExtra)\n\n# Twitter data charts\n#Most Frequently Single words (unigram) (in alphabetically order)\nChart_OneToken_twitter <- ggplot(MostFreq_OneToken_twitter, aes(x=Word,y=Frequency)) \n  + geom_bar(stat=\"Identity\", fill=\"green\") \n  + geom_text(aes(label=Frequency), vjust=-0.20) \n  + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently biwords (2-grams) (ordered alphabetically)\nChart_BiToken_twitter <- ggplot(MostFreq_BiToken_twitter, aes(x=Word,y=Frequency)) \n  + geom_bar(stat=\"Identity\", fill=\"green\") \n  + geom_text(aes(label=Frequency), vjust=-0.20) \n  + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently triwords (3-grams) (ordered alphabetically)\nChart_TriToken_twitter <- ggplot(MostFreq_TriToken_twitter, aes(x=Word,y=Frequency)) \n  + geom_bar(stat=\"Identity\", fill=\"green\") \n  + geom_text(aes(label=Frequency), vjust=-0.20) \n  + theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngrid.arrange(Chart_OneToken_twitter, Chart_BiToken_twitter, Chart_TriToken_twitter, ncol=3)\n\n# Blogs data charts\n#Most Frequently Single words (unigram) (in alphabetically order)\nChart_OneToken_blogs <- ggplot(MostFreq_OneToken_blogs, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently biwords (2-grams) (ordered alphabetically)\nChart_BiToken_blogs <- ggplot(MostFreq_BiToken_blogs, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently triwords (3-grams) (ordered alphabetically)\nChart_TriToken_blogs <- ggplot(MostFreq_TriToken_blogs, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngrid.arrange(Chart_OneToken_blogs, Chart_BiToken_blogs, Chart_TriToken_blogs, ncol=3)\n\n\n\n# News data charts\n#Most Frequently Single words (unigram) (in alphabetically order)\nChart_OneToken_news <- ggplot(MostFreq_OneToken_news, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently biwords (2-grams) (ordered alphabetically)\nChart_BiToken_news <- ggplot(MostFreq_BiToken_news, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n#Most Frequently triwords (3-grams) (ordered alphabetically)\nChart_TriToken_news <- ggplot(MostFreq_TriToken_news, aes(x=Word,y=Frequency)) \n+ geom_bar(stat=\"Identity\", fill=\"green\") \n+ geom_text(aes(label=Frequency), vjust=-0.20) \n+ theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\ngrid.arrange(Chart_OneToken_news, Chart_BiToken_news, Chart_TriToken_news, ncol=3)\n\n\n\n\n########################################################################################################\n##### What comes next\n#######################################################################################################\nWe can see that in both three data sets, the most frequent n-grams tends to be the same. It makes us thing about \ncombining all the data to one big Corpus. But from now that even individual data are heavy to load, the next stem is to\nthink about a parralel compution system. A training/test/validation sets will also be a good help toward good prediction\nmodel.\nAt the end, some diving into javascript autosugestion will be good for the final product. We are hoping to get at the end to \na userfriendly app.\n\n\n\n",
    "created" : 1429854643706.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1616236573",
    "id" : "CDE4D6F2",
    "lastKnownWriteTime" : 1427665199,
    "path" : "F:/Cours/Coursera/The Data Science Specialization/Module 10 - Capstone Project/report draft.R",
    "project_path" : null,
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}