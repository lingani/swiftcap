{
    "contents" : "library(swiftcap)\ndata (blogs)\nfit <- ngram (blogs, N=3)\npredict (fit, \"What is the next\")\n\n\n\nN           = 1:3\nfreq_cutoff = 1\nrank_cutoff = 5\ndelimiters  = ' \\r\\n\\t.,;:\\\\\"()?!'\n\n\n# transform the text input into clean sentences\nsentences <- clean_sentences (split_by_sentence (blogs))\n\n# split the sentences into ngrams\nngrams <- split_by_ngram (sentences, min (N), max (N), delimiters)\n\n# determine the frequency/count of each phrase\nngrams <- ngrams [, list (frequency = .N), by = phrase]\n\n\n\n# extract the context and the next word for each ngram\nngrams [, word    := last_word (phrase),        by = phrase]\nngrams [, context := except_last_word (phrase), by = phrase]\n\n# calculate the MLE of the probability of occurrence for each n-gram\ncontext <- ngrams [, sum (frequency), by = context]\nhead(context)\nsetnames (context, c(\"context\", \"context_frequency\"))\n\n# through merging of context and ngrams, calculate the probability\nsetkeyv (context, \"context\")\nsetkeyv (ngrams, \"context\")\n\n# calculate the maximum liklihood estimate\nngrams [context, p := frequency / context_frequency]\nhead(ngrams, 30)\n\n\nngrams <- ngram_mle (ngrams)\n\n\n# exclude ngrams that are below the frequency cut-off\nngrams <- ngrams [ frequency >= freq_cutoff, list (phrase, context, word, p) ]\n\n# do not predict a 'start of sentence'\nngrams <- ngrams [word != \"^\"]\n\n# do not predict 'end of sentence' with no context or at the start of a sentence\nngrams <- ngrams [!(context == \"\"  & word == \"$\")]\nngrams <- ngrams [!(context == \"^\" & word == \"$\")]\n\n\n# mark each n-gram as a 1, 2, ... N gram\nregex <- paste0 (\"[\", delimiters, \"]+\")\nngrams [, n := unlist (lapply (stri_split (phrase, regex = regex), length)) ]\n\n# keep only most likely words for each context\nngrams <- ngrams [ order (context, -p)]\nngrams [, rank := 1:.N, by = context]\nngrams <- ngrams [ rank <= rank_cutoff ]\n\n\nmodel <- list (ngrams      = ngrams,\n               N           = N,\n               freq_cutoff = freq_cutoff,\n               rank_cutoff = rank_cutoff)\nclass (model) <- \"ngram\"\n\nphrase <- \"What is the next\"\nwords <- split_by_word (clean_sentences (split_by_sentence (phrase)))\n\n\nif (!stri_detect (phrase, regex = \".*[\\\\.!?][[:blank:]]*$\"))\n    words <- head (words, -1)\n\npredictions <- NULL\nobject <- fit\n\nfor (n in sort (object$N, decreasing = TRUE)) {\n    n = 1\n    # ensure there are enough previous words\n    # for example, a trigram ngrams needs 2 previous words\n    if (length (words) >= n-1) {\n\n        print(words) #ML\n        # grab the necessary context; last 'n-1' words\n        ctx <- paste (tail (words, n-1), collapse = \" \")\n\n        # find matching context in the model\n        predictions <- object$ngrams [ context == ctx, list (word, p, n, rank)]\n        if (nrow (predictions) > 0) {\n\n            # basic translations\n            # TODO ugly; should handle in a better way\n            predictions [word == \"$\", word := \".\"]\n            predictions [word == \"###\", word := NA]\n\n            # exclude any missing predictions\n            predictions <- predictions [complete.cases (predictions)]\n\n            # only keep the top 'rank' predictions\n            predictions <- predictions [rank <= rank]\n\n            break\n        }\n    }\n}\n\nreturn (predictions)\n",
    "created" : 1429572249052.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1936340603",
    "id" : "BDF24CC8",
    "lastKnownWriteTime" : 1429646322,
    "path" : "~/swiftcap/draft_prediction.R",
    "project_path" : "draft_prediction.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "source_on_save" : false,
    "type" : "r_source"
}